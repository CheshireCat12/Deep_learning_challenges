{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PGGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "x0nrCQ8ds-GN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ixALv_ZtEM9",
        "colab_type": "code",
        "outputId": "9e2076e5-0225-4cf9-eb6b-1502aff40b43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "cell_type": "code",
      "source": [
        "# Image preprocessing\n",
        "transform = transforms.Compose([transforms.Resize(32),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=(0.5, ),\n",
        "                                                     std=(0.5, ))])\n",
        "\n",
        "\n",
        "fasion_mnist = datasets.FashionMNIST(root=\"./data\",\n",
        "                                     train=True,\n",
        "                                     transform=transform,\n",
        "                                     download=True)\n",
        "\n",
        "# Parameters\n",
        "params_loader = {'batch_size': 64,\n",
        "                 'shuffle': False}\n",
        "\n",
        "train_loader = DataLoader(fasion_mnist, **params_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "26427392it [00:07, 3768389.22it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 102345.58it/s]           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4423680it [00:01, 3859783.02it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 33584.18it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eCRnCyVUtN6r",
        "colab_type": "code",
        "outputId": "80e2077e-29c8-4798-8627-c643f2d149b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3345
        }
      },
      "cell_type": "code",
      "source": [
        "# Generator model\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, init_depth=1024, ncc=3):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        self.z_dim = z_dim\n",
        "        self.init_depth = init_depth\n",
        "        self.ncc = ncc\n",
        "        \n",
        "        self.model = self._create_model()\n",
        "        \n",
        "        # resolution at the output of the original network\n",
        "        self.resl = 8\n",
        "        self.dim = self.init_depth // 2\n",
        "        \n",
        "    \n",
        "    def _block(self, ch_in, ch_out, ks=(4, 4), stride=2, padding=1, last=False):\n",
        "        block = [nn.ConvTranspose2d(in_channels=ch_in,\n",
        "                                    out_channels=ch_out,\n",
        "                                    kernel_size=ks,\n",
        "                                    stride=stride,\n",
        "                                    padding=padding,\n",
        "                                    bias=False)]\n",
        "        if not last:\n",
        "            block += [nn.BatchNorm2d(ch_out),\n",
        "                      nn.ReLU()]\n",
        "        else:\n",
        "            block += [nn.Tanh()]\n",
        "            \n",
        "        return block\n",
        "    \n",
        "    def _init_layers(self):\n",
        "        layers = [*self._block(self.z_dim, self.init_depth, stride=1, padding=0),\n",
        "                  *self._block(self.init_depth, self.init_depth)]\n",
        "        \n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def _to_rgb(self, dim):\n",
        "        to_rgb = self._block(dim,\n",
        "                             self.ncc,\n",
        "                             ks=(5, 5),\n",
        "                             stride=1,\n",
        "                             padding=2,\n",
        "                             last=True)\n",
        "        return nn.Sequential(*to_rgb)\n",
        "        \n",
        "    \n",
        "    def _create_model(self):\n",
        "        init_layers = self._init_layers()\n",
        "        to_rgb = self._to_rgb(self.init_depth)\n",
        "        \n",
        "        model = nn.Sequential()\n",
        "        model.add_module(\"init_layers\", init_layers)\n",
        "        model.add_module(\"to_rgb\", to_rgb)\n",
        "        \n",
        "        return model\n",
        "    \n",
        "    def grow_network(self):\n",
        "        old_resl = self.resl\n",
        "        self.resl *= 2\n",
        "        growing = f\"{old_resl}x{old_resl}->{self.resl}x{self.resl}\"\n",
        "        \n",
        "        print(f\"Growing the generator network : {growing}, can take few seconds...\")\n",
        "        \n",
        "        self.layer_name = \"layer_\" + growing\n",
        "        \n",
        "        # Create the new model\n",
        "        new_model = nn.Sequential()\n",
        "        deep_copy_model(self.model, new_model, [\"to_rgb\"], not_is_in)\n",
        "        \n",
        "        # Add this layer to avoid a too brusk change when a new layer is added.\n",
        "        hist_to_rgb = nn.Sequential()\n",
        "        deep_copy_model(self.model, hist_to_rgb, [\"to_rgb\"], is_in)\n",
        "        \n",
        "        # Add this layer to avoid a too brusk change when a new layer is added.\n",
        "        transition_to_rgb = nn.Sequential()\n",
        "        transition_to_rgb.add_module(\"upsample_to_rgb\",\n",
        "                                     nn.Upsample(scale_factor=2, mode='nearest'))\n",
        "        transition_to_rgb.add_module(\"hist_from_rgb\", hist_to_rgb)\n",
        "        \n",
        "        # TODO: find a better way to handle the channels' dim\n",
        "        dim_in = self.dim * 2\n",
        "        dim_out = self.dim\n",
        "        \n",
        "        # Add the new layer on the top of the descriminator\n",
        "        new_layer = nn.Sequential()\n",
        "        new_layer.add_module(\"new_layer\", nn.Sequential(*self._block(dim_in,\n",
        "                                                                     dim_out)))\n",
        "        new_layer.add_module(\"new_to_rgb\", self._to_rgb(dim_out))\n",
        "        \n",
        "        \n",
        "        new_model.add_module(\"concatenate_block\",\n",
        "                             ConcatenateLayers(transition_to_rgb,\n",
        "                                               new_layer))\n",
        "        new_model.add_module(\"fadein\", FadeIn())\n",
        "        \n",
        "        \n",
        "        self.model = new_model\n",
        "        \n",
        "    def clean_network(self):\n",
        "        new_model = nn.Sequential()\n",
        "        \n",
        "        deep_copy_model(self.model,\n",
        "                        new_model, \n",
        "                        [\"concatenate_block\", \"fadein\"],\n",
        "                        not_is_in)\n",
        "        \n",
        "        rgb_layer, fadedIn_layer = nn.Sequential(), nn.Sequential()\n",
        "        deep_copy_model(self.model.concatenate_block.new_layer,\n",
        "                        rgb_layer,\n",
        "                        [\"new_to_rgb\"],\n",
        "                        is_in)\n",
        "        deep_copy_model(self.model.concatenate_block.new_layer,\n",
        "                        fadedIn_layer,\n",
        "                        [\"new_layer\"],\n",
        "                        is_in)\n",
        "        \n",
        "        new_model.add_module(self.layer_name, fadedIn_layer.new_layer)\n",
        "        new_model.add_module(\"to_rgb\", rgb_layer.new_to_rgb)\n",
        "        \n",
        "        self.model = new_model\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.model(x)\n",
        "\n",
        "z_dim = 100\n",
        "net_g = Generator(z_dim, ncc=1)\n",
        "net_g.grow_network()\n",
        "print(net_g)\n",
        "net_g.clean_network()\n",
        "print(net_g)\n",
        "net_g.grow_network()\n",
        "print(net_g)\n",
        "net_g.clean_network()\n",
        "print(net_g)\n",
        "fixed_noise = torch.randn(params_loader[\"batch_size\"], z_dim, 1, 1)\n",
        "val = net_g(fixed_noise)\n",
        "print(val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Growing the generator network : 8x8->16x16, can take few seconds...\n",
            "Generator(\n",
            "  (model): Sequential(\n",
            "    (init_layers): Sequential(\n",
            "      (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU()\n",
            "    )\n",
            "    (concatenate_block): ConcatenateLayers(\n",
            "      (trans_layer): Sequential(\n",
            "        (upsample_to_rgb): Upsample(scale_factor=2, mode=nearest)\n",
            "        (hist_from_rgb): Sequential(\n",
            "          (to_rgb): Sequential(\n",
            "            (0): ConvTranspose2d(1024, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "            (1): Tanh()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (new_layer): Sequential(\n",
            "        (new_layer): Sequential(\n",
            "          (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU()\n",
            "        )\n",
            "        (new_to_rgb): Sequential(\n",
            "          (0): ConvTranspose2d(512, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "          (1): Tanh()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (fadein): FadeIn()\n",
            "  )\n",
            ")\n",
            "Generator(\n",
            "  (model): Sequential(\n",
            "    (init_layers): Sequential(\n",
            "      (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU()\n",
            "    )\n",
            "    (layer_8x8->16x16): Sequential(\n",
            "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (to_rgb): Sequential(\n",
            "      (0): ConvTranspose2d(512, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (1): Tanh()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Growing the generator network : 16x16->32x32, can take few seconds...\n",
            "Generator(\n",
            "  (model): Sequential(\n",
            "    (init_layers): Sequential(\n",
            "      (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU()\n",
            "    )\n",
            "    (layer_8x8->16x16): Sequential(\n",
            "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (concatenate_block): ConcatenateLayers(\n",
            "      (trans_layer): Sequential(\n",
            "        (upsample_to_rgb): Upsample(scale_factor=2, mode=nearest)\n",
            "        (hist_from_rgb): Sequential(\n",
            "          (to_rgb): Sequential(\n",
            "            (0): ConvTranspose2d(512, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "            (1): Tanh()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (new_layer): Sequential(\n",
            "        (new_layer): Sequential(\n",
            "          (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU()\n",
            "        )\n",
            "        (new_to_rgb): Sequential(\n",
            "          (0): ConvTranspose2d(512, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "          (1): Tanh()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (fadein): FadeIn()\n",
            "  )\n",
            ")\n",
            "Generator(\n",
            "  (model): Sequential(\n",
            "    (init_layers): Sequential(\n",
            "      (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU()\n",
            "    )\n",
            "    (layer_8x8->16x16): Sequential(\n",
            "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (layer_16x16->32x32): Sequential(\n",
            "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (to_rgb): Sequential(\n",
            "      (0): ConvTranspose2d(512, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (1): Tanh()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-46adaf004672>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_g\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0mfixed_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_loader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_g\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-46adaf004672>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0mz_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    755\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    756\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [1024, 512, 4, 4], expected input[64, 512, 16, 16] to have 1024 channels, but got 512 channels instead"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "uS7f5ktxvgsH",
        "colab_type": "code",
        "outputId": "24d298b2-fad1-4a87-8474-7bf1dd12164f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2153
        }
      },
      "cell_type": "code",
      "source": [
        "# Discriminator model\n",
        "\n",
        "def is_in(name, module_name):\n",
        "    return name in module_name\n",
        "\n",
        "def not_is_in(name, module_name):\n",
        "    return name not in module_name\n",
        "\n",
        "# ref: https://github.com/nashory/pggan-pytorch/blob/master/network.py\n",
        "def deep_copy_model(model, new_model, module_name, is_inside):\n",
        "    \"\"\"\n",
        "    Return the module of the model given in parameters,\n",
        "    if not_ is True return all the module not in the module_name list.\n",
        "    Args:\n",
        "        model (torch.nn.Module): model to copy\n",
        "        new_model (torch.nn.Module): The copied model.\n",
        "        module_name (list(string)): module name to conserve during the copy\n",
        "        is_inside (Callable): Include or exclude modules to save\n",
        "    \"\"\"\n",
        "    for name, m in model.named_children():\n",
        "        if is_inside(name, module_name):\n",
        "            new_model.add_module(name, m)\n",
        "            new_model[-1].load_state_dict(m.state_dict())\n",
        "            \n",
        "\n",
        "class ConcatenateLayers(nn.Module):\n",
        "    def __init__(self, trans_layer, new_layer):\n",
        "        super(ConcatenateLayers, self).__init__()\n",
        "        \n",
        "        self.trans_layer = trans_layer\n",
        "        self.new_layer = new_layer\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return (self.trans_layer(x), self.new_layer(x))\n",
        "    \n",
        "class FadeIn(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FadeIn, self).__init__()\n",
        "        \n",
        "        self.alpha = 0.\n",
        "        \n",
        "    def update_alpha(self, delta):\n",
        "        self.alpha += delta\n",
        "        self.alpha = max(0., min(self.alpha, 1.))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (tuple):\n",
        "                val_from_trans (torch.Tensor): Tensor from the transition layer\n",
        "                val_from_new (torch.Tensor): Tensor from the last added layer\n",
        "        \n",
        "        Return:\n",
        "            Fade in the tensor comming from the last added layer.\n",
        "            Formula: (1-alpha)*trans + alpha* new\n",
        "        \"\"\"\n",
        "        val_from_trans, val_from_new = x\n",
        "        out = torch.add((1.-self.alpha) * val_from_trans,\n",
        "                        self.alpha * val_from_new)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_size, ncc=3, init_depth=64, max_depth=1024):\n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        self.img_size = img_size\n",
        "        self.ncc = ncc\n",
        "        self.init_depth = init_depth\n",
        "        self.max_depth = max_depth\n",
        "        self.resl = 8\n",
        "        \n",
        "        self.model = self._create_model()\n",
        "        self.dim = self.max_depth // 2\n",
        "        \n",
        "    def _create_model(self):\n",
        "        model = nn.Sequential()\n",
        "        \n",
        "        rgb_layer = self._from_rgb(self.max_depth)\n",
        "        init_layers = self._init_layers()\n",
        "        \n",
        "        model.add_module(\"rgb_layer\", rgb_layer)\n",
        "        model.add_module(\"init_layers\", init_layers)\n",
        "        \n",
        "        return model\n",
        "        \n",
        "    def _block(self, depth_in, depth_out, last=False, stride=2, padding=1):\n",
        "        block = [nn.Conv2d(in_channels=depth_in,\n",
        "                           out_channels=depth_out,\n",
        "                           kernel_size=(3, 3),\n",
        "                           stride=stride,\n",
        "                           padding=padding,\n",
        "                           bias=False)]\n",
        "        if not last:\n",
        "            block += [nn.BatchNorm2d(depth_out),\n",
        "                      nn.LeakyReLU(0.2)]\n",
        "        \n",
        "        return block\n",
        "    \n",
        "    def _init_layers(self):\n",
        "        internal_layers = [*self._block(self.max_depth, self.max_depth),\n",
        "                           # Last layer different from the others : depth x 4 x 4 -> 1 x 1 x 1\n",
        "                           *self._block(self.max_depth, 1, True, 2, 0)]\n",
        "        \n",
        "        return nn.Sequential(*internal_layers)\n",
        "    \n",
        "    def _from_rgb(self, dim):\n",
        "        rgb_layer = self._block(self.ncc, dim)\n",
        "        \n",
        "        return nn.Sequential(*rgb_layer)\n",
        "    \n",
        "    def grow_network(self):\n",
        "        \"\"\"Add a new layer on the top of the descriminator.\"\"\"\n",
        "        old_resl = self.resl\n",
        "        self.resl *= 2\n",
        "        growing = f\"{self.resl}x{self.resl}->{old_resl}x{old_resl}\"\n",
        "        \n",
        "        print(f\"Growing the discriminator network : {growing}, can take few seconds...\")\n",
        "        \n",
        "        self.layer_name = \"layer_\" + growing\n",
        "        \n",
        "        \n",
        "        # TODO: find a better way to handle the channels' dim\n",
        "        dim_in = self.dim\n",
        "        dim_out = self.dim * 2\n",
        "        \n",
        "        hist_from_rgb = nn.Sequential()\n",
        "        deep_copy_model(self.model, hist_from_rgb, [\"rgb_layer\"], is_in)\n",
        "        \n",
        "        # Add this layer to avoid a too brusk change when a new layer is added.\n",
        "        transition_from_rgb = nn.Sequential()\n",
        "        transition_from_rgb.add_module(\"downsample_from_rgb\",\n",
        "                                       nn.AvgPool2d(kernel_size=2))\n",
        "        transition_from_rgb.add_module(\"hist_from_rgb\", hist_from_rgb)\n",
        "        \n",
        "        # Add the new layer on the top of the descriminator\n",
        "        new_layer = nn.Sequential()\n",
        "        new_layer.add_module(\"new_from_rgb\", self._from_rgb(dim_in))\n",
        "        new_layer.add_module(\"new_layer\", nn.Sequential(*self._block(dim_in,\n",
        "                                                                     dim_out)))\n",
        "        \n",
        "        # Create the new model\n",
        "        new_model = nn.Sequential()\n",
        "        new_model.add_module(\"concatenate_block\",\n",
        "                             ConcatenateLayers(transition_from_rgb,\n",
        "                                               new_layer))\n",
        "        new_model.add_module(\"fadein\", FadeIn())\n",
        "        \n",
        "        deep_copy_model(self.model, new_model, [\"rgb_layer\"], not_is_in)\n",
        "        \n",
        "        self.model = new_model\n",
        "        self.dim //= 2\n",
        "        \n",
        "    def clean_network(self):\n",
        "        \"\"\"Once the new layer is completly fade in, remove the useless layers.\"\"\"\n",
        "        rgb_layer, fadedIn_layer = nn.Sequential(), nn.Sequential()\n",
        "        deep_copy_model(self.model.concatenate_block.new_layer,\n",
        "                        rgb_layer,\n",
        "                        [\"new_from_rgb\"],\n",
        "                        is_in)\n",
        "        deep_copy_model(self.model.concatenate_block.new_layer,\n",
        "                        fadedIn_layer,\n",
        "                        [\"new_layer\"],\n",
        "                        is_in)\n",
        "        \n",
        "        new_model = nn.Sequential()\n",
        "        new_model.add_module(\"rgb_layer\", rgb_layer.new_from_rgb)\n",
        "        new_model.add_module(self.layer_name, fadedIn_layer.new_layer)\n",
        "        \n",
        "        deep_copy_model(self.model,\n",
        "                        new_model,\n",
        "                        [\"concatenate_block\", \"fadein\"],\n",
        "                        not_is_in)\n",
        "\n",
        "        self.model = new_model\n",
        "    \n",
        "    def forward(self, input_):\n",
        "        output = self.model(input_)\n",
        "\n",
        "        return output.view(-1, 1).squeeze(1)\n",
        "    \n",
        "net_d = Discriminator(4, 1)\n",
        "# print(net_d)\n",
        "net_d.grow_network()\n",
        "print(net_d)\n",
        "net_d.clean_network()\n",
        "print(net_d)\n",
        "net_d.grow_network()\n",
        "print(net_d)\n",
        "net_d.clean_network()\n",
        "print(net_d)\n",
        "for img, _ in train_loader:\n",
        "    val = net_d(img)\n",
        "    \n",
        "    print(val.shape)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Growing the discriminator network : 16x16->8x8, can take few seconds...\n",
            "Discriminator(\n",
            "  (model): Sequential(\n",
            "    (concatenate_block): ConcatenateLayers(\n",
            "      (trans_layer): Sequential(\n",
            "        (downsample_from_rgb): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "        (hist_from_rgb): Sequential(\n",
            "          (rgb_layer): Sequential(\n",
            "            (0): Conv2d(1, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): LeakyReLU(negative_slope=0.2)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (new_layer): Sequential(\n",
            "        (new_from_rgb): Sequential(\n",
            "          (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): LeakyReLU(negative_slope=0.2)\n",
            "        )\n",
            "        (new_layer): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): LeakyReLU(negative_slope=0.2)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (fadein): FadeIn()\n",
            "    (init_layers): Sequential(\n",
            "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "      (3): Conv2d(1024, 1, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Discriminator(\n",
            "  (model): Sequential(\n",
            "    (rgb_layer): Sequential(\n",
            "      (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (layer_16x16->8x8): Sequential(\n",
            "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (init_layers): Sequential(\n",
            "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "      (3): Conv2d(1024, 1, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Growing the discriminator network : 32x32->16x16, can take few seconds...\n",
            "Discriminator(\n",
            "  (model): Sequential(\n",
            "    (concatenate_block): ConcatenateLayers(\n",
            "      (trans_layer): Sequential(\n",
            "        (downsample_from_rgb): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "        (hist_from_rgb): Sequential(\n",
            "          (rgb_layer): Sequential(\n",
            "            (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (2): LeakyReLU(negative_slope=0.2)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (new_layer): Sequential(\n",
            "        (new_from_rgb): Sequential(\n",
            "          (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): LeakyReLU(negative_slope=0.2)\n",
            "        )\n",
            "        (new_layer): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): LeakyReLU(negative_slope=0.2)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (fadein): FadeIn()\n",
            "    (layer_16x16->8x8): Sequential(\n",
            "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (init_layers): Sequential(\n",
            "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "      (3): Conv2d(1024, 1, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Discriminator(\n",
            "  (model): Sequential(\n",
            "    (rgb_layer): Sequential(\n",
            "      (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (layer_32x32->16x16): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (layer_16x16->8x8): Sequential(\n",
            "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (init_layers): Sequential(\n",
            "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "      (3): Conv2d(1024, 1, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mgKL4IGavjGO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://github.com/pytorch/examples/blob/master/dcgan/main.py\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iXDjs1IGvk37",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dimension of the latent space\n",
        "Z_dim = 100\n",
        "\n",
        "# Number of color channel in the final image\n",
        "ncc = 3\n",
        "\n",
        "# Number of internal node\n",
        "ngf, ndf = 64, 64\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KvAC2McyvtZy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}